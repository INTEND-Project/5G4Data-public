apiVersion: batch/v1
kind: Job
metadata:
  name: rusty-llm-inference-test
  namespace: rusty-llm
  labels:
    app: rusty-llm-inference-test
spec:
  ttlSecondsAfterFinished: 300  # Clean up after 5 minutes
  backoffLimit: 2
  template:
    metadata:
      labels:
        app: rusty-llm-inference-test
    spec:
      restartPolicy: Never
      containers:
      - name: inference-test
        image: python:3.11-slim
        command:
        - /bin/bash
        - -c
        - |
          set -e
          pip install -q requests > /dev/null 2>&1
          
          python3 << 'PYTHON_SCRIPT'
          import requests
          import json
          import sys
          import os
          
          namespace = os.environ.get('NAMESPACE', 'rusty-llm')
          service_name = os.environ.get('SERVICE_NAME', 'rusty-llm')
          service_port = os.environ.get('SERVICE_PORT', '8080')
          test_question = os.environ.get('TEST_QUESTION', 'What is 2+2?')
          
          url = f"http://{service_name}.{namespace}:{service_port}/v1/chat/completions"
          
          print("=== Rusty-LLM Model Inference Test ===")
          print(f"Service URL: {url}")
          print(f"Question: {test_question}")
          print("")
          
          # Prepare the request
          payload = {
              "stream": True,
              "model": "rusty_llm",
              "messages": [
                  {"role": "user", "content": test_question}
              ]
          }
          
          headers = {
              "Content-Type": "application/json"
          }
          
          print("Sending request to model...")
          print("")
          
          try:
              # Make the streaming request
              response = requests.post(
                  url,
                  json=payload,
                  headers=headers,
                  stream=True,
                  timeout=180  # Increased timeout for model inference
              )
              
              # Check status code
              if response.status_code != 200:
                  print(f"✗ Request failed with status {response.status_code}")
                  print(f"Response: {response.text[:500]}")
                  sys.exit(1)
              
              response.raise_for_status()
              
              print("✓ Connection successful")
              print("")
              print("=== Streaming Response ===")
              
              # Parse SSE stream
              full_response = ""
              chunk_count = 0
              
              for line in response.iter_lines():
                  if not line:
                      continue
                  
                  line_str = line.decode('utf-8')
                  
                  # Skip non-data lines
                  if not line_str.startswith('data:'):
                      continue
                  
                  # Extract JSON data
                  data_str = line_str[5:].strip()  # Remove 'data: ' prefix
                  
                  if data_str == '[DONE]':
                      break
                  
                  try:
                      data = json.loads(data_str)
                      chunk_count += 1
                      
                      # Extract content from delta
                      if 'choices' in data and len(data['choices']) > 0:
                          choice = data['choices'][0]
                          if 'delta' in choice and 'content' in choice['delta']:
                              content = choice['delta']['content']
                              full_response += content
                              # Print content as it arrives
                              print(content, end='', flush=True)
                      
                  except json.JSONDecodeError as e:
                      # Skip invalid JSON
                      continue
              
              print("")
              print("")
              print(f"Received {chunk_count} chunks")
              print("")
              
              if not full_response:
                  print("✗ No content received in response")
                  sys.exit(1)
              
              print("=== Full Model Response ===")
              print(full_response)
              print("")
              print(f"Response length: {len(full_response)} characters")
              print("")
              
              if len(full_response) < 5:
                  print("⚠ Warning: Response seems very short")
              else:
                  print("✓ Model inference is working correctly")
                  print("")
                  print("=== Test Result: SUCCESS ===")
              
          except requests.exceptions.Timeout:
              print("✗ Request timed out (model may be taking too long to respond)")
              sys.exit(1)
          except requests.exceptions.ConnectionError as e:
              print(f"✗ Connection error: {e}")
              print("")
              print("Possible causes:")
              print("1. The service is not running or not accessible")
              print("2. The embedding model (model/embed.gguf) is missing")
              print("   - Check pod logs: kubectl logs -l app.kubernetes.io/component=ai-server -n rusty-llm")
              print("   - The embedding model is required for chat completions")
              print("   - Ensure embed.gguf is included in the Docker image")
              sys.exit(1)
          except requests.exceptions.RequestException as e:
              print(f"✗ Request failed: {e}")
              if hasattr(e, 'response') and e.response is not None:
                  print(f"Response status: {e.response.status_code}")
                  print(f"Response body: {e.response.text[:500]}")
              sys.exit(1)
          except Exception as e:
              print(f"✗ Error: {e}")
              sys.exit(1)
          PYTHON_SCRIPT
        env:
        - name: NAMESPACE
          value: "rusty-llm"
        - name: SERVICE_NAME
          value: "rusty-llm"
        - name: SERVICE_PORT
          value: "8080"
        - name: TEST_QUESTION
          value: "What is 2+2? Explain briefly."

